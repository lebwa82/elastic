{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .\" \n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "#output = model(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input.input_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(t.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_len(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    res = encoded_input.input_ids.shape[1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_text(text, max_len = 500):\n",
    "    while get_token_len(text) > max_len:\n",
    "        print(text)\n",
    "        chunks = text.split(' ')\n",
    "        chunks.pop()\n",
    "        text = ' '.join(chunks)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"Nadim Ladki ++++++++++++++++++++++++++++++++ On 13 October 2021 , the newspaper said its online news coverage had been temporarily suspended due to \\\" circumstances beyond our control .\\\" Several weeks later , on 31 October 2021 , its editor - in - chief Nadim Ladki told employees that the newspaper was officially shutting down . Some of the writers and reporters who contributed to The Daily Star include Mirella Hodeib , Wassim Mroueh , Hussein Dakroub , Jim Quilty , Ryan Shultz , Alex Rowell , Joseph Haboush , Venetia Rainey , Olivia Alabaster , Samya Kullab , Ghinwa Obeid , Benjamin Redd , Abby Sewell , Kareem Shaheen , Nizar Hassan , Edy Semaan , Justin Salhani , Heba Nasser , Timour Azhari , Emily Lewis , Beckie Strum , Hashem Osseiran , Dana Khraiche , Susan Wilson , Mazin Sidahmed , Lizzie Porter , Meris Lutz , and Ryan Stultz . Some of the photojournalists that contributed to the newspaper include Hasan Shaaban and Mohamad Azakir . ++++++++++++++++++++++++++++++++ Hang Nadim International Airport ( Bandar Udara Internasional Hang Nadim ) BTH is an international airport located in Batam , Riau Islands , Indonesia . It is named after Laksamana Hang Nadim Pahlawan Kechik , a legendary warrior from the region . The airport is the primary method of transport to and from Batam , alongside ferries to neighboring islands , including the sovereign city - state Singapore in the north . ++++++++++++++++++++++++++++++++ Recently the airport has become an important hub for the aircraft maintenance , repair , and overhaul ( MRO ) industry . In the long term , Hang Nadim Airport is planned to be developed into an Aeropolis covering an area of 1 , 800 hectares . Lion Air ' s subsidiary Batam Aero Technic ( BAT ) has invested in Hang Nadim by building MRO facilities . BAT plans to expand the existing hangar to 28 hectares to accommodate as many as 250 aircraft . ++++++++++++++++++++++++++++++++ Nadim Gemayel started his struggle against the Syrians by participating and leading Anti - Syrian demonstrations , organizing sit - ins with other Anti - Syrian factions inside the universities , and lobbying for Lebanon during his university years outside Lebanon . ++++++++++++++++++++++++++++++++ Since 1973 , Doncaster has been the home of the Doncaster Youth Jazz Association ( DYJA ). Founded by John\"\n",
    "get_token_len(t)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_len('Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART-\n"
     ]
    }
   ],
   "source": [
    "print(cut_text('-DOCSTART-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 101, 4715, 2900,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "text = 'SOCCER JAPAN'\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' this', ' this is', ' this is a', ' this is a foo', ' is', ' is a', ' is a foo', ' is a foo bar', ' a', ' a foo', ' a foo bar', ' a foo bar sentences', ' foo', ' foo bar', ' foo bar sentences', ' foo bar sentences and', ' bar', ' bar sentences', ' bar sentences and', ' bar sentences and i', ' sentences', ' sentences and', ' sentences and i', ' sentences and i want', ' and', ' and i', ' and i want', ' and i want to', ' i', ' i want', ' i want to', ' i want to ngramize', ' want', ' want to', ' want to ngramize', ' want to ngramize it', ' to', ' to ngramize', ' to ngramize it', ' ngramize', ' ngramize it', ' it']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [['SOCCER', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']]\n",
    "all_span_word = [[['SOCCER'], ['SOCCER', 'JAPAN'], ['SOCCER', 'JAPAN', 'GET'], ['SOCCER', 'JAPAN', 'GET', 'LUCKY'], ['JAPAN'], ['JAPAN', 'GET'], ['JAPAN', 'GET', 'LUCKY'], ['JAPAN', 'GET', 'LUCKY', 'WIN'], ['GET'], ['GET', 'LUCKY'], ['GET', 'LUCKY', 'WIN'], ['GET', 'LUCKY', 'WIN', ','], ['LUCKY'], ['LUCKY', 'WIN'], ['LUCKY', 'WIN', ','], ['LUCKY', 'WIN', ',', 'CHINA'], ['WIN'], ['WIN', ','], ['WIN', ',', 'CHINA'], ['WIN', ',', 'CHINA', 'IN'], [','], [',', 'CHINA'], [',', 'CHINA', 'IN'], [',', 'CHINA', 'IN', 'SURPRISE'], ['CHINA'], ['CHINA', 'IN'], ['CHINA', 'IN', 'SURPRISE'], ['CHINA', 'IN', 'SURPRISE', 'DEFEAT'], ['IN'], ['IN', 'SURPRISE'], ['IN', 'SURPRISE', 'DEFEAT'], ['IN', 'SURPRISE', 'DEFEAT', '.'], ['SURPRISE'], ['SURPRISE', 'DEFEAT'], ['SURPRISE', 'DEFEAT', '.'], ['DEFEAT'], ['DEFEAT', '.'], ['.']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOCCER', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN']\n",
      "[['SOCCER'], ['SOCCER', 'JAPAN'], ['SOCCER', 'JAPAN', 'GET'], ['SOCCER', 'JAPAN', 'GET', 'LUCKY'], ['JAPAN'], ['JAPAN', 'GET'], ['JAPAN', 'GET', 'LUCKY'], ['JAPAN', 'GET', 'LUCKY', 'WIN'], ['GET'], ['GET', 'LUCKY'], ['GET', 'LUCKY', 'WIN'], ['GET', 'LUCKY', 'WIN', ','], ['LUCKY'], ['LUCKY', 'WIN'], ['LUCKY', 'WIN', ','], ['LUCKY', 'WIN', ',', 'CHINA'], ['WIN'], ['WIN', ','], ['WIN', ',', 'CHINA'], ['WIN', ',', 'CHINA', 'IN'], [','], [',', 'CHINA'], [',', 'CHINA', 'IN'], ['CHINA'], ['CHINA', 'IN'], ['IN']]\n",
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "my_words = words[0][:8]\n",
    "print(my_words)\n",
    "my_size = len(my_words)\n",
    "N = 4\n",
    "my_ngrams = []\n",
    "i = 0\n",
    "j = 0\n",
    "while i < my_size:\n",
    "    j = i\n",
    "    gram = []\n",
    "    #my_ngrams.append(gram)\n",
    "    while j < my_size and j < i+ N:\n",
    "        gram.append(my_words[j])\n",
    "        my_ngrams.append(gram.copy())\n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "print(my_ngrams)\n",
    "print(len(my_ngrams))\n",
    "count = 0\n",
    "my_predicts = []\n",
    "for i in range(len(all_span_word[0])):\n",
    "    if all_span_word[0][i] in my_ngrams:\n",
    "        #my_predicts.append(my_ngrams[i])\n",
    "        count += 1\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
    "len(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_size = len(my_words)\n",
    "N = 4\n",
    "my_ngrams = []\n",
    "i = 0\n",
    "j = 0\n",
    "while i < my_size:\n",
    "    j = i\n",
    "    gram = []\n",
    "    #my_ngrams.append(gram)\n",
    "    while j < my_size and j < i + N:\n",
    "        gram.append(my_words[j])\n",
    "        my_ngrams.append(gram.copy())\n",
    "        j += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(my_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SOCCER'], ['SOCCER', '-'], ['SOCCER', '-', 'JAPAN'], ['SOCCER', '-', 'JAPAN', 'GET'], ['-'], ['-', 'JAPAN'], ['-', 'JAPAN', 'GET'], ['-', 'JAPAN', 'GET', 'LUCKY'], ['JAPAN'], ['JAPAN', 'GET'], ['JAPAN', 'GET', 'LUCKY'], ['JAPAN', 'GET', 'LUCKY', 'WIN'], ['GET'], ['GET', 'LUCKY'], ['GET', 'LUCKY', 'WIN'], ['GET', 'LUCKY', 'WIN', ','], ['LUCKY'], ['LUCKY', 'WIN'], ['LUCKY', 'WIN', ','], ['LUCKY', 'WIN', ',', 'CHINA'], ['WIN'], ['WIN', ','], ['WIN', ',', 'CHINA'], ['WIN', ',', 'CHINA', 'IN'], [','], [',', 'CHINA'], [',', 'CHINA', 'IN'], [',', 'CHINA', 'IN', 'SURPRISE'], ['CHINA'], ['CHINA', 'IN'], ['CHINA', 'IN', 'SURPRISE'], ['CHINA', 'IN', 'SURPRISE', 'DEFEAT'], ['IN'], ['IN', 'SURPRISE'], ['IN', 'SURPRISE', 'DEFEAT'], ['IN', 'SURPRISE', 'DEFEAT', '.'], ['SURPRISE'], ['SURPRISE', 'DEFEAT'], ['SURPRISE', 'DEFEAT', '.'], ['DEFEAT'], ['DEFEAT', '.'], ['.']]\n"
     ]
    }
   ],
   "source": [
    "print(my_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "predicts = tensor([[[1.0000e+00, 1.5581e-14, 8.0536e-15, 6.5220e-14, 9.3445e-14]]],\n",
    "       device='cuda:0')\n",
    "\n",
    "my_predicts = tensor([[1.0000e+00, 1.5581e-14, 8.0536e-15, 6.5220e-14, 9.3445e-14]],\n",
    "       device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.tolist() == my_predicts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0,\n",
       "   1.558099996408751e-14,\n",
       "   8.053600273922202e-15,\n",
       "   6.521999872674891e-14,\n",
       "   9.344500000174616e-14]]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  1.558099996408751e-14,\n",
       "  8.053600273922202e-15,\n",
       "  6.521999872674891e-14,\n",
       "  9.344500000174616e-14]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predicts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dima/elastic/tokens.ipynb Ячейка 22\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dima/elastic/tokens.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predicts\u001b[39m.\u001b[39mtolist() \u001b[39m-\u001b[39m my_predicts\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_span_word = [[['SOCCER'], ['SOCCER', '-'], ['SOCCER', '-', 'JAPAN'], ['SOCCER', '-', 'JAPAN', 'GET'], ['-'], ['-', 'JAPAN'], ['-', 'JAPAN', 'GET'], ['-', 'JAPAN', 'GET', 'LUCKY'], ['JAPAN'], ['JAPAN', 'GET'], ['JAPAN', 'GET', 'LUCKY'], ['JAPAN', 'GET', 'LUCKY', 'WIN'], ['GET'], ['GET', 'LUCKY'], ['GET', 'LUCKY', 'WIN'], ['GET', 'LUCKY', 'WIN', ','], ['LUCKY'], ['LUCKY', 'WIN'], ['LUCKY', 'WIN', ','], ['LUCKY', 'WIN', ',', 'CHINA'], ['WIN'], ['WIN', ','], ['WIN', ',', 'CHINA'], ['WIN', ',', 'CHINA', 'IN'], [','], [',', 'CHINA'], [',', 'CHINA', 'IN'], [',', 'CHINA', 'IN', 'SURPRISE'], ['CHINA'], ['CHINA', 'IN'], ['CHINA', 'IN', 'SURPRISE'], ['CHINA', 'IN', 'SURPRISE', 'DEFEAT'], ['IN'], ['IN', 'SURPRISE'], ['IN', 'SURPRISE', 'DEFEAT'], ['IN', 'SURPRISE', 'DEFEAT', '.'], ['SURPRISE'], ['SURPRISE', 'DEFEAT'], ['SURPRISE', 'DEFEAT', '.'], ['SURPRISE', 'DEFEAT', '.', 'The'], ['DEFEAT'], ['DEFEAT', '.'], ['DEFEAT', '.', 'The'], ['DEFEAT', '.', 'The', 'qualification'], ['.'], ['.', 'The'], ['.', 'The', 'qualification'], ['.', 'The', 'qualification', 'campaign'], ['The'], ['The', 'qualification'], ['The', 'qualification', 'campaign'], ['The', 'qualification', 'campaign', 'for'], ['qualification'], ['qualification', 'campaign'], ['qualification', 'campaign', 'for'], ['qualification', 'campaign', 'for', 'the'], ['campaign'], ['campaign', 'for'], ['campaign', 'for', 'the'], ['campaign', 'for', 'the', '2011'], ['for'], ['for', 'the'], ['for', 'the', '2011'], ['for', 'the', '2011', 'Asian'], ['the'], ['the', '2011'], ['the', '2011', 'Asian'], ['the', '2011', 'Asian', 'Cup'], ['2011'], ['2011', 'Asian'], ['2011', 'Asian', 'Cup'], ['2011', 'Asian', 'Cup', 'was'], ['Asian'], ['Asian', 'Cup'], ['Asian', 'Cup', 'was'], ['Asian', 'Cup', 'was', 'acceptable'], ['Cup'], ['Cup', 'was'], ['Cup', 'was', 'acceptable'], ['Cup', 'was', 'acceptable', 'for'], ['was'], ['was', 'acceptable'], ['was', 'acceptable', 'for'], ['was', 'acceptable', 'for', 'Yemeni'], ['acceptable'], ['acceptable', 'for'], ['acceptable', 'for', 'Yemeni'], ['acceptable', 'for', 'Yemeni', \"'\"], ['for'], ['for', 'Yemeni'], ['for', 'Yemeni', \"'\"], ['for', 'Yemeni', \"'\", 'standards'], ['Yemeni'], ['Yemeni', \"'\"], ['Yemeni', \"'\", 'standards'], ['Yemeni', \"'\", 'standards', '.'], [\"'\"], [\"'\", 'standards'], [\"'\", 'standards', '.'], [\"'\", 'standards', '.', 'Despite'], ['standards'], ['standards', '.'], ['standards', '.', 'Despite'], ['standards', '.', 'Despite', 'being'], ['.'], ['.', 'Despite'], ['.', 'Despite', 'being'], ['.', 'Despite', 'being', 'grouped'], ['Despite'], ['Despite', 'being'], ['Despite', 'being', 'grouped'], ['Despite', 'being', 'grouped', 'with'], ['being'], ['being', 'grouped'], ['being', 'grouped', 'with'], ['being', 'grouped', 'with', 'Japan'], ['grouped'], ['grouped', 'with'], ['grouped', 'with', 'Japan'], ['grouped', 'with', 'Japan', 'and'], ['with'], ['with', 'Japan'], ['with', 'Japan', 'and'], ['with', 'Japan', 'and', 'Bahrain'], ['Japan'], ['Japan', 'and'], ['Japan', 'and', 'Bahrain'], ['Japan', 'and', 'Bahrain', ','], ['and'], ['and', 'Bahrain'], ['and', 'Bahrain', ','], ['and', 'Bahrain', ',', 'and'], ['Bahrain'], ['Bahrain', ','], ['Bahrain', ',', 'and'], ['Bahrain', ',', 'and', 'Hong'], [','], [',', 'and'], [',', 'and', 'Hong'], [',', 'and', 'Hong', 'Kong'], ['and'], ['and', 'Hong'], ['and', 'Hong', 'Kong'], ['and', 'Hong', 'Kong', ','], ['Hong'], ['Hong', 'Kong'], ['Hong', 'Kong', ','], ['Hong', 'Kong', ',', 'they'], ['Kong'], ['Kong', ','], ['Kong', ',', 'they'], ['Kong', ',', 'they', 'achieved'], [','], [',', 'they'], [',', 'they', 'achieved'], [',', 'they', 'achieved', 'two'], ['they'], ['they', 'achieved'], ['they', 'achieved', 'two'], ['they', 'achieved', 'two', 'wins'], ['achieved'], ['achieved', 'two'], ['achieved', 'two', 'wins'], ['achieved', 'two', 'wins', ','], ['two'], ['two', 'wins'], ['two', 'wins', ','], ['two', 'wins', ',', 'one'], ['wins'], ['wins', ','], ['wins', ',', 'one'], ['wins', ',', 'one', 'draw'], [','], [',', 'one'], [',', 'one', 'draw'], [',', 'one', 'draw', 'and'], ['one'], ['one', 'draw'], ['one', 'draw', 'and'], ['one', 'draw', 'and', 'three'], ['draw'], ['draw', 'and'], ['draw', 'and', 'three'], ['draw', 'and', 'three', 'losses'], ['and'], ['and', 'three'], ['and', 'three', 'losses'], ['and', 'three', 'losses', '.'], ['three'], ['three', 'losses'], ['three', 'losses', '.'], ['three', 'losses', '.', 'They'], ['losses'], ['losses', '.'], ['losses', '.', 'They'], ['losses', '.', 'They', 'opened'], ['.'], ['.', 'They'], ['.', 'They', 'opened'], ['.', 'They', 'opened', 'with'], ['They'], ['They', 'opened'], ['They', 'opened', 'with'], ['They', 'opened', 'with', 'a'], ['opened'], ['opened', 'with'], ['opened', 'with', 'a'], ['opened', 'with', 'a', 'surprise'], ['with'], ['with', 'a'], ['with', 'a', 'surprise'], ['with', 'a', 'surprise', 'narrow'], ['a'], ['a', 'surprise'], ['a', 'surprise', 'narrow'], ['a', 'surprise', 'narrow', '-'], ['surprise'], ['surprise', 'narrow'], ['surprise', 'narrow', '-'], ['surprise', 'narrow', '-', 'defeat'], ['narrow'], ['narrow', '-'], ['narrow', '-', 'defeat'], ['narrow', '-', 'defeat', 'of'], ['-'], ['-', 'defeat'], ['-', 'defeat', 'of'], ['-', 'defeat', 'of', '21'], ['defeat'], ['defeat', 'of'], ['defeat', 'of', '21'], ['defeat', 'of', '21', 'to'], ['of'], ['of', '21'], ['of', '21', 'to'], ['of', '21', 'to', 'Japan'], ['21'], ['21', 'to'], ['21', 'to', 'Japan'], ['21', 'to', 'Japan', 'and'], ['to'], ['to', 'Japan'], ['to', 'Japan', 'and'], ['to', 'Japan', 'and', 'finished'], ['Japan'], ['Japan', 'and'], ['Japan', 'and', 'finished'], ['Japan', 'and', 'finished', 'with'], ['and'], ['and', 'finished'], ['and', 'finished', 'with'], ['and', 'finished', 'with', 'the'], ['finished'], ['finished', 'with'], ['finished', 'with', 'the'], ['finished', 'with', 'the', 'surprise'], ['with'], ['with', 'the'], ['with', 'the', 'surprise'], ['with', 'the', 'surprise', 'of'], ['the'], ['the', 'surprise'], ['the', 'surprise', 'of'], ['the', 'surprise', 'of', ','], ['surprise'], ['surprise', 'of'], ['surprise', 'of', ','], ['surprise', 'of', ',', 'once'], ['of'], ['of', ','], ['of', ',', 'once'], ['of', ',', 'once', 'again'], [','], [',', 'once'], [',', 'once', 'again'], [',', 'once', 'again', ','], ['once'], ['once', 'again'], ['once', 'again', ','], ['once', 'again', ',', 'holding'], ['again'], ['again', ','], ['again', ',', 'holding'], ['again', ',', 'holding', 'Japan'], [','], [',', 'holding'], [',', 'holding', 'Japan'], [',', 'holding', 'Japan', 'to'], ['holding'], ['holding', 'Japan'], ['holding', 'Japan', 'to'], ['holding', 'Japan', 'to', 'the'], ['Japan'], ['Japan', 'to'], ['Japan', 'to', 'the'], ['Japan', 'to', 'the', 'last'], ['to'], ['to', 'the'], ['to', 'the', 'last'], ['to', 'the', 'last', 'minute'], ['the'], ['the', 'last'], ['the', 'last', 'minute'], ['the', 'last', 'minute', 'for'], ['last'], ['last', 'minute'], ['last', 'minute', 'for'], ['last', 'minute', 'for', 'a'], ['minute'], ['minute', 'for'], ['minute', 'for', 'a'], ['minute', 'for', 'a', '32'], ['for'], ['for', 'a'], ['for', 'a', '32'], ['for', 'a', '32', 'defeat'], ['a'], ['a', '32'], ['a', '32', 'defeat'], ['a', '32', 'defeat', '.'], ['32'], ['32', 'defeat'], ['32', 'defeat', '.'], ['32', 'defeat', '.', 'Elizabeth'], ['defeat'], ['defeat', '.'], ['defeat', '.', 'Elizabeth'], ['defeat', '.', 'Elizabeth', 'and'], ['.'], ['.', 'Elizabeth'], ['.', 'Elizabeth', 'and'], ['.', 'Elizabeth', 'and', 'Lucky'], ['Elizabeth'], ['Elizabeth', 'and'], ['Elizabeth', 'and', 'Lucky'], ['Elizabeth', 'and', 'Lucky', 'help'], ['and'], ['and', 'Lucky'], ['and', 'Lucky', 'help'], ['and', 'Lucky', 'help', 'Nikolas'], ['Lucky'], ['Lucky', 'help'], ['Lucky', 'help', 'Nikolas'], ['Lucky', 'help', 'Nikolas', 'and'], ['help'], ['help', 'Nikolas'], ['help', 'Nikolas', 'and'], ['help', 'Nikolas', 'and', 'Emily'], ['Nikolas'], ['Nikolas', 'and'], ['Nikolas', 'and', 'Emily'], ['Nikolas', 'and', 'Emily', 'evade'], ['and'], ['and', 'Emily'], ['and', 'Emily', 'evade'], ['and', 'Emily', 'evade', 'Helena'], ['Emily'], ['Emily', 'evade'], ['Emily', 'evade', 'Helena'], ['Emily', 'evade', 'Helena', ','], ['evade'], ['evade', 'Helena'], ['evade', 'Helena', ','], ['evade', 'Helena', ',', 'and'], ['Helena'], ['Helena', ','], ['Helena', ',', 'and'], ['Helena', ',', 'and', 'get'], [','], [',', 'and'], [',', 'and', 'get'], [',', 'and', 'get', 'back'], ['and'], ['and', 'get'], ['and', 'get', 'back'], ['and', 'get', 'back', 'together'], ['get'], ['get', 'back'], ['get', 'back', 'together'], ['get', 'back', 'together', '.'], ['back'], ['back', 'together'], ['back', 'together', '.'], ['back', 'together', '.', 'Their'], ['together'], ['together', '.'], ['together', '.', 'Their'], ['together', '.', 'Their', 'financial'], ['.'], ['.', 'Their'], ['.', 'Their', 'financial'], ['.', 'Their', 'financial', 'struggles'], ['Their'], ['Their', 'financial'], ['Their', 'financial', 'struggles'], ['Their', 'financial', 'struggles', 'prompt'], ['financial'], ['financial', 'struggles'], ['financial', 'struggles', 'prompt'], ['financial', 'struggles', 'prompt', 'Elizabeth'], ['struggles'], ['struggles', 'prompt'], ['struggles', 'prompt', 'Elizabeth'], ['struggles', 'prompt', 'Elizabeth', 'to'], ['prompt'], ['prompt', 'Elizabeth'], ['prompt', 'Elizabeth', 'to'], ['prompt', 'Elizabeth', 'to', 'become'], ['Elizabeth'], ['Elizabeth', 'to'], ['Elizabeth', 'to', 'become'], ['Elizabeth', 'to', 'become', 'a'], ['to'], ['to', 'become'], ['to', 'become', 'a'], ['to', 'become', 'a', 'surrogate'], ['become'], ['become', 'a'], ['become', 'a', 'surrogate'], ['become', 'a', 'surrogate', 'mother'], ['a'], ['a', 'surrogate'], ['a', 'surrogate', 'mother'], ['a', 'surrogate', 'mother', 'for'], ['surrogate'], ['surrogate', 'mother'], ['surrogate', 'mother', 'for'], ['surrogate', 'mother', 'for', 'Courtney'], ['mother'], ['mother', 'for'], ['mother', 'for', 'Courtney'], ['mother', 'for', 'Courtney', 'Matthews'], ['for'], ['for', 'Courtney'], ['for', 'Courtney', 'Matthews'], ['for', 'Courtney', 'Matthews', '('], ['Courtney'], ['Courtney', 'Matthews'], ['Courtney', 'Matthews', '('], ['Courtney', 'Matthews', '(', 'Alicia'], ['Matthews'], ['Matthews', '('], ['Matthews', '(', 'Alicia'], ['('], ['(', 'Alicia'], ['Alicia']]]\n",
    "my_ngrams = [['SOCCER'], ['SOCCER', '-'], ['SOCCER', '-', 'JAPAN'], ['SOCCER', '-', 'JAPAN', 'GET'], ['-'], ['-', 'JAPAN'], ['-', 'JAPAN', 'GET'], ['-', 'JAPAN', 'GET', 'LUCKY'], ['JAPAN'], ['JAPAN', 'GET'], ['JAPAN', 'GET', 'LUCKY'], ['JAPAN', 'GET', 'LUCKY', 'WIN'], ['GET'], ['GET', 'LUCKY'], ['GET', 'LUCKY', 'WIN'], ['GET', 'LUCKY', 'WIN', ','], ['LUCKY'], ['LUCKY', 'WIN'], ['LUCKY', 'WIN', ','], ['LUCKY', 'WIN', ',', 'CHINA'], ['WIN'], ['WIN', ','], ['WIN', ',', 'CHINA'], ['WIN', ',', 'CHINA', 'IN'], [','], [',', 'CHINA'], [',', 'CHINA', 'IN'], [',', 'CHINA', 'IN', 'SURPRISE'], ['CHINA'], ['CHINA', 'IN'], ['CHINA', 'IN', 'SURPRISE'], ['CHINA', 'IN', 'SURPRISE', 'DEFEAT'], ['IN'], ['IN', 'SURPRISE'], ['IN', 'SURPRISE', 'DEFEAT'], ['IN', 'SURPRISE', 'DEFEAT', '.'], ['SURPRISE'], ['SURPRISE', 'DEFEAT'], ['SURPRISE', 'DEFEAT', '.'], ['DEFEAT'], ['DEFEAT', '.'], ['.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "my_predicts = []\n",
    "for i in range(len(all_span_word[0])):\n",
    "    if all_span_word[0][i] in my_ngrams:\n",
    "        my_ngrams.remove(all_span_word[0][i])\n",
    "        #my_predicts.append(predicts_list[i])\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-']\n",
      "[',']\n",
      "['.']\n",
      "['for']\n",
      "['the']\n",
      "['for']\n",
      "['.']\n",
      "['with']\n",
      "['Japan']\n",
      "['Japan', 'and']\n",
      "['and']\n",
      "[',']\n",
      "[',', 'and']\n",
      "['and']\n",
      "[',']\n",
      "[',']\n",
      "['and']\n",
      "['.']\n",
      "['with']\n",
      "['a']\n",
      "['surprise']\n",
      "['-']\n",
      "['defeat']\n",
      "['of']\n",
      "['to']\n",
      "['Japan']\n",
      "['Japan', 'and']\n",
      "['and']\n",
      "['with']\n",
      "['the']\n",
      "['surprise']\n",
      "['of']\n",
      "[',']\n",
      "[',']\n",
      "['Japan']\n",
      "['to']\n",
      "['the']\n",
      "['for']\n",
      "['a']\n",
      "['defeat']\n",
      "['.']\n",
      "['Elizabeth']\n",
      "['and']\n",
      "['and']\n",
      "[',']\n",
      "[',', 'and']\n",
      "['and']\n",
      "['.']\n",
      "['Elizabeth']\n",
      "['to']\n",
      "['a']\n",
      "['for']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_span_word[0])):\n",
    "    copy = all_span_word[0].copy()\n",
    "    copy.pop(i)\n",
    "    if all_span_word[0][i] in copy:\n",
    "        print(all_span_word[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
